
.. _MCMC:


************************
Markov chain Monte Carlo
************************


Monte Carlo
===========


*Monte Carlo* (term coined by S. Ulam and N. Metropolis) refers to the method of 
generating random numbers from a certain distribution. 

.. For archaeological applications, 

For dealing with :ref:`temporal uncertainty <uncertainty>`, for example, a Monte Carlo method consists on 
creating artificial "potential" time series within the boundary of existence, and then 
assessing the likelihood of the variable's character state over time. 



Markov chain
============

A *Markov chain* (after A. Markov) is a sequence of numbers where each number is dependent on the previous 
number in the sequence.


To simulate a Markov chain whose equilibrium distribution is a posterior density :math:`p(x)` defined on
the set of possible configurations of a system, we use Markov chain Monte Carlo (MCMC) methods.


Some MCMC algorithms: 

* Metropolis-Hastings
* Gibbs sampling
* Random walk Metropolis  


Alternative to MCMC:

* Importance/rejection sampling (used to evaluate the probability of rare events)


|


MCMC is used to fit a model and to draw samples from the joint posterior distribution of the model parameters.


* (see :ref:`Bayesian statistics <bayes>`)



|


Metropolis-Hastings
===================

The *Metropolis-Hastings* algorithm serves to generate a sample from the posterior distribution 
of :math:`\theta`, which is the parameter of the data point's distribution. 


|


Algorithm
---------

1. Initialise :math:`x^{(0)}`

2. For :math:`i=0` to :math:`N-1`

     Sample :math:`u \sim \mathcal{U}(0, 1)`

     Sample :math:`x^\star \sim q(x^\star \mid x^{(i)})`

     If :math:`u < \mathcal{U}() = \min\{1, \frac{p^\star \; q(x^{(i)}) \mid x^\star}{x^{(i)}) \; q(x^\star \mid x^{(i)})}\}`
            
            :math:`x^{(i+1)} = x^\star`
            
     Else   
            
            :math:`x^{(i+1)} = x^{(i)}`



|


Diagnostics
===========

Diagnostics is to assess the MCMC performance.

The trace plot (or time-series plot) is to judge how quickly the MCMC procedure converges in distribution. 

When the variability is not the same over all iterations, the trace plot presents a "random walk" pattern. 




|


Burn-in
--------

First generate a series of random numbers from a normal distribution with a mean value and some arbitrary variance. 


    .. todo:: 
    
        Throwing away early samples is called burn-in...


|


MCMC Simulation
---------------

    .. todo:: 
    
        After the burn-in, are the MCM iterations...






|


Thinning
--------

    .. todo:: 
    
        Thinning interval for reducing autocorrelation of random points in the sampling...


|


Convergence
-----------

Convergence in MCMC is obtained when these outcomes become minimal with the increase of :math:`n`:

* the change in the variance
* the standard error


|


.. _bayes:


Bayesian statistics
===================

Markov chain Monte Carlo is a tool for sampling from distributions with high dimensionality, 
which is a need typically found in some Bayesian statistics contexts. 

Bayesian analysis implies Bayesian inference, model fitting, and performing inferential and 
predictive summaries with predictive checks. 

As mentioned in :ref:`temporal uncertainty <uncertainty>`, the subjective approach in 
assigning probabilities includes a certain degree of belief or knowledge to the probability, 
and this is to arrive at a posterior distribution to updated the state of knowledge about the 
parameters. 



|


.. _bayesT:


Bayes theorem
-------------

Key concepts in Bayesian statistics are related in Bayes theorem, which is a rule 
that provides the conditional probability of :math:`\theta` (model or hypothesis) 
occurring given that :math:`x` (data or observation) already happened.  

:math:`P(\theta \mid x) \;=\; \frac{ P(\theta)~P(x \mid \theta)  } { P(x)  }` 

That is, the posterior probability of :math:`\theta` given :math:`x` where

* :math:`P(\theta \mid x)` is the **posterior** probability of :math:`\theta`
* :math:`P(\theta)` is the **prior** probability of :math:`\theta`
* :math:`P(x \mid \theta)` is the **likelihood** of :math:`\theta`
* :math:`P(x)` is the **unconditional** probability of :math:`x`


Hence, the posterior is an updated degree of belief or knowledge based on the prior 
with known probabilities. This is our posterior *target* distribution sometimes written 
as :math:`P_{target}(\theta)`.

On the other hand, the likelihood is the probability that the hypothesis confers upon 
the observation, and the probability of the observation irrespective of any hypothesis 
is the unconditional probability of the observation. 


.. We are interested in the probability given all of the observations, and Bayes theorem 



|


.. :math:`\theta =` the probability of 



.. meta::
   :description: Monte Carlo methods and Markov chain
   :keywords: Bayesian, MCMC

